{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "from datetime import date\n",
    "import string\n",
    "import unicodedata\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rreplace(s, old, new, n_occurrence):\n",
    "    # Replace the last n_occurrence (n_occurrence = 1 is the last occurrence) of an expression in a string s\n",
    "    li = s.rsplit(old, n_occurrence)\n",
    "    return new.join(li)\n",
    "\n",
    "def str_format(stringg, sep = '_'):\n",
    "    \n",
    "    #Elimina simbolos de puntuación, u otros simbolos de un string\n",
    "    #Usar sep = \"_\" para nombre de columnas, usar sep = ' ' para contenido\n",
    "    \n",
    "    punctuation = list(string.punctuation)          \n",
    "    symbols = ['^','°', 'ª', os.linesep, '\\n']                     \n",
    "    new_string = unicodedata.normalize('NFKD', stringg).encode('ascii', errors='ignore').decode('utf-8') # Elimina tildes \n",
    "\n",
    "    if sep == '_':\n",
    "        for i in range(len(punctuation)):\n",
    "            new_string = new_string.replace(punctuation[i], sep)   # Elimina simbolos de puntuación\n",
    "    else:\n",
    "        new_string = new_string.replace(';','.')\n",
    "    \n",
    "    for i in range(len(symbols)):\n",
    "        new_string = new_string.replace(symbols[i], '')           # Elimina otros simbolos\n",
    "    \n",
    "    if sep == '_':\n",
    "        new_string = new_string.replace(' ','_').lower()         # Elimina espacios \n",
    "    \n",
    "    while new_string[-1:] == sep:\n",
    "        new_string = rreplace(new_string,sep,'',1)           # Elimina los \"espacios\" (_) al final del string\n",
    "    \n",
    "    return new_string\n",
    "\n",
    "\n",
    "def read_txt(path, header_int, sep = '|', encoding = 'latin1'):\n",
    "# Lee los archivos txt del path (correspondientes a la fecha de descarga), y crea diccionario con respectivos dataframes\n",
    "    all_files = pd.Series(glob.glob(path + \"/*.txt\"))\n",
    "    fecha_str = date.today().strftime(\"%Y%m\")\n",
    "    files = all_files[all_files.astype(str).str.contains(fecha_str)].tolist()\n",
    "    d = {}\n",
    "    \n",
    "    for i in range(0,len(files)):\n",
    "        file_path = files[i]\n",
    "        index_from = file_path.rfind(\"\\\\\") + 1\n",
    "        file_name = str_format(file_path[index_from:-4])\n",
    "        \n",
    "        if file_name[:8].isdigit():\n",
    "            file_name = file_name[9:]\n",
    "        \n",
    "        d[str(file_name)] = pd.read_csv(file_path,header=header_int, sep=sep, encoding=encoding, dtype=str)\n",
    "    return d\n",
    "\n",
    "def clean_columns(dict, del_from, col=0):\n",
    "    for keys in dict:\n",
    "        # Elimina la primera y última columna \n",
    "        dict[keys] = dict[keys].drop(columns=dict[keys].columns[0]).\\\n",
    "        drop(columns = dict[keys].columns[-1]).dropna(how = 'all').reset_index(drop = True)\n",
    "        del_from_index = dict[keys].index[dict[keys][dict[keys].columns[col]].str.contains(del_from)].tolist()[0]\n",
    "        \n",
    "        #Elimina filas al final de txt que presentan el resumen del libro de compras/ventas\n",
    "        dict[keys] = dict[keys].iloc[0:del_from_index,:]\n",
    "    return dict\n",
    "\n",
    "def save_as_csv(dict, path, suffixes='', sep=';', encoding = 'latin1', index = False):\n",
    "    \n",
    "    for keys in dict:\n",
    "        dict[keys].to_csv(path + '\\\\' + dt.now().strftime('%Y%m%d') + str(keys) + suffixes + '.csv', sep=sep, index=index)\n",
    "        \n",
    "def read_xlsx(path, header_int, sheet_name_str=0):\n",
    "# Lee los archivos xlsx del path (correspondientes a la fecha de descarga), y crea diccionario con respectivos dataframes\n",
    "    all_files = pd.Series(glob.glob(path + \"/*.xlsx\"))\n",
    "    fecha_str = date.today().strftime(\"%Y%m\")\n",
    "    files = all_files[all_files.astype(str).str.contains(fecha_str)].tolist()\n",
    "    d = {}\n",
    "    \n",
    "    for i in range(0,len(files)):\n",
    "        file_path = files[i]\n",
    "        index_from = file_path.rfind(\"\\\\\") + 1\n",
    "        file_name = str_format(file_path[index_from:-4])\n",
    "        \n",
    "        if file_name[:8].isdigit():\n",
    "            file_name = file_name[9:]\n",
    "        \n",
    "        d[str(file_name)] = pd.read_excel(file_path, header=header_int, sheet_name=sheet_name_str, dtype=str)\n",
    "    return d\n",
    "\n",
    "def col_names_format(dict):\n",
    "    \n",
    "    for keys in dict:\n",
    "        df_col = dict[keys].columns\n",
    "        col_list = []\n",
    "        for col in df_col:\n",
    "            col_list.append(str_format(col))\n",
    "        dict[keys].columns = col_list\n",
    "    \n",
    "    return dict\n",
    "\n",
    "def schema_cols(dict, col_names, empty_col_value = np.nan):\n",
    "    for keys in dict:\n",
    "        for col in col_names:\n",
    "            if col not in dict[keys].columns:\n",
    "                dict[keys][col] = empty_col_value\n",
    "        dict[keys] = dict[keys][col_names]\n",
    "    return dict\n",
    "\n",
    "def replace_unnamed_cols(dict):\n",
    "    columns = []\n",
    "    for keys in dict:\n",
    "        for col in dict[keys].columns:\n",
    "            if 'Unnamed' in str(col):\n",
    "                col = ''\n",
    "            columns.append(col)\n",
    "    dict[keys].columns = columns\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutar limpieza\n",
    "### Leer archivos y guardar en diccionario dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mrc = read_xlsx(r'C:/projects/data_lake/data/input/current/pentana',2,\"Matriz de Riesgo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cambiar formato de nombre de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mrc = col_names_format(dict_mrc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establecer columnas que serán parte del esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['unnamed__0', 'proceso', 'subproceso', 'riesgo_relevante', 'actividad',\n",
    "       'numero_de_riesgo', 'que_puede_fallar', 'como_puede_fallar',\n",
    "       'quien_puede_fallar', 'donde_puede_fallar', 'riesgo_especifico',\n",
    "       'unnamed__11', 'participacion_en_el_mercado', 'reputacion', 'economico',\n",
    "       'legal', 'medio_ambiente__rse',\n",
    "       'perjuicio_a_colaboradores_o_beneficiarios', 'operativa__continuidad',\n",
    "       'unnamed__19', 'tipo_de_proceso', 'recursos', 'frecuencia', 'masividad',\n",
    "       'unnamed__24', 'impacto', 'probabilidad', 'nivel', 'unnamed__28',\n",
    "       'area_que_levanta_el_riesgo', 'incorporacion_en_el_programa_de_trabajo',\n",
    "       'unnamed__31', 'unnamed__32']\n",
    "\n",
    "dict_mrc = schema_cols(dict_mrc, col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rellenar valores nulos con \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keys in dict_mrc:\n",
    "    dict_mrc[keys].fillna('', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cambiar formato de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keys in dict_mrc:\n",
    "    dict_mrc[keys] = dict_mrc[keys].applymap(lambda x: str_format(x, sep=' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar en archivos csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_csv(dict_mrc, r'C:/projects/data_lake/data/output/current/pentana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
